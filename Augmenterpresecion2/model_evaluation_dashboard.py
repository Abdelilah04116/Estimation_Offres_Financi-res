"""
Dashboard d'√âvaluation Complet des Mod√®les ML
Inclut : m√©triques d√©taill√©es, visualisations avanc√©es, analyses par segment
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from sklearn.metrics import (roc_curve, precision_recall_curve, confusion_matrix,
                            classification_report, mean_squared_error, r2_score,
                            mean_absolute_error)
from sklearn.inspection import permutation_importance
import shap
import warnings
warnings.filterwarnings('ignore')

class ModelEvaluationDashboard:
    """Dashboard complet d'√©valuation des mod√®les"""
    
    def __init__(self, models_data, X_test, y_test, feature_names=None):
        self.models_data = models_data
        self.X_test = X_test
        self.y_test = y_test
        self.feature_names = feature_names or [f'feature_{i}' for i in range(X_test.shape[1])]
        self.results = {}
        
    def evaluate_classification_models(self):
        """√âvalue tous les mod√®les de classification"""
        print("üìä √âvaluation des mod√®les de classification...")
        
        classification_results = {}
        
        for name, model_data in self.models_data.items():
            if 'classification' in name.lower() or 'clf' in name.lower():
                model = model_data['model']
                y_pred = model_data['predictions']
                y_pred_proba = model_data.get('probabilities')
                
                # M√©triques de base
                metrics = {
                    'roc_auc': roc_auc_score(self.y_test, y_pred_proba),
                    'accuracy': (y_pred == self.y_test).mean(),
                    'precision': classification_report(self.y_test, y_pred, output_dict=True)['1']['precision'],
                    'recall': classification_report(self.y_test, y_pred, output_dict=True)['1']['recall'],
                    'f1_score': classification_report(self.y_test, y_pred, output_dict=True)['1']['f1-score']
                }
                
                # Courbes ROC et PR
                fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)
                precision, recall, _ = precision_recall_curve(self.y_test, y_pred_proba)
                
                # Matrice de confusion
                cm = confusion_matrix(self.y_test, y_pred)
                
                classification_results[name] = {
                    'metrics': metrics,
                    'roc_curve': (fpr, tpr),
                    'pr_curve': (precision, recall),
                    'confusion_matrix': cm,
                    'predictions': y_pred,
                    'probabilities': y_pred_proba
                }
                
                print(f"‚úÖ {name}: AUC={metrics['roc_auc']:.4f}, Acc={metrics['accuracy']:.4f}")
        
        self.results['classification'] = classification_results
        return classification_results
    
    def evaluate_regression_models(self):
        """√âvalue tous les mod√®les de r√©gression"""
        print("üìä √âvaluation des mod√®les de r√©gression...")
        
        regression_results = {}
        
        for name, model_data in self.models_data.items():
            if 'regression' in name.lower() or 'reg' in name.lower():
                model = model_data['model']
                y_pred = model_data['predictions']
                
                # M√©triques de base
                metrics = {
                    'r2': r2_score(self.y_test, y_pred),
                    'rmse': np.sqrt(mean_squared_error(self.y_test, y_pred)),
                    'mae': mean_absolute_error(self.y_test, y_pred),
                    'mse': mean_squared_error(self.y_test, y_pred)
                }
                
                # R√©sidus
                residuals = self.y_test - y_pred
                
                regression_results[name] = {
                    'metrics': metrics,
                    'predictions': y_pred,
                    'residuals': residuals,
                    'actual': self.y_test
                }
                
                print(f"‚úÖ {name}: R¬≤={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}")
        
        self.results['regression'] = regression_results
        return regression_results
    
    def create_classification_visualizations(self):
        """Cr√©e les visualisations pour la classification"""
        print("üìà Cr√©ation des visualisations de classification...")
        
        if 'classification' not in self.results:
            print("‚ùå R√©sultats de classification non disponibles")
            return
        
        # Configuration du style
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('Dashboard d\'√âvaluation - Classification', fontsize=16, fontweight='bold')
        
        # 1. Courbes ROC
        ax = axes[0, 0]
        for name, result in self.results['classification'].items():
            fpr, tpr = result['roc_curve']
            auc = result['metrics']['roc_auc']
            ax.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})')
        
        ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)
        ax.set_xlabel('Taux de Faux Positifs')
        ax.set_ylabel('Taux de Vrais Positifs')
        ax.set_title('Courbes ROC')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. Courbes Precision-Recall
        ax = axes[0, 1]
        for name, result in self.results['classification'].items():
            precision, recall = result['pr_curve']
            ax.plot(recall, precision, label=name)
        
        ax.set_xlabel('Recall')
        ax.set_ylabel('Precision')
        ax.set_title('Courbes Precision-Recall')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. Comparaison des m√©triques
        ax = axes[0, 2]
        metrics_df = pd.DataFrame([
            result['metrics'] for result in self.results['classification'].values()
        ], index=self.results['classification'].keys())
        
        metrics_df[['roc_auc', 'accuracy', 'precision', 'recall', 'f1_score']].plot(
            kind='bar', ax=ax, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
        )
        ax.set_title('Comparaison des M√©triques')
        ax.set_ylabel('Score')
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3)
        
        # 4. Matrices de confusion (premier mod√®le)
        ax = axes[1, 0]
        first_model = list(self.results['classification'].keys())[0]
        cm = self.results['classification'][first_model]['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
        ax.set_title(f'Matrice de Confusion - {first_model}')
        ax.set_xlabel('Pr√©dictions')
        ax.set_ylabel('Valeurs R√©elles')
        
        # 5. Distribution des probabilit√©s
        ax = axes[1, 1]
        for name, result in self.results['classification'].items():
            probs = result['probabilities']
            ax.hist(probs, bins=30, alpha=0.7, label=name, density=True)
        
        ax.set_xlabel('Probabilit√© Pr√©dite')
        ax.set_ylabel('Densit√©')
        ax.set_title('Distribution des Probabilit√©s')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 6. Feature Importance (premier mod√®le)
        ax = axes[1, 2]
        first_model = list(self.results['classification'].keys())[0]
        model = self.models_data[first_model]['model']
        
        if hasattr(model, 'feature_importances_'):
            importance = model.feature_importances_
            top_features = np.argsort(importance)[-10:]
            
            ax.barh(range(len(top_features)), importance[top_features])
            ax.set_yticks(range(len(top_features)))
            ax.set_yticklabels([self.feature_names[i] for i in top_features])
            ax.set_xlabel('Importance')
            ax.set_title(f'Feature Importance - {first_model}')
        
        plt.tight_layout()
        plt.savefig('classification_evaluation.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return fig
    
    def create_regression_visualizations(self):
        """Cr√©e les visualisations pour la r√©gression"""
        print("üìà Cr√©ation des visualisations de r√©gression...")
        
        if 'regression' not in self.results:
            print("‚ùå R√©sultats de r√©gression non disponibles")
            return
        
        # Configuration du style
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('Dashboard d\'√âvaluation - R√©gression', fontsize=16, fontweight='bold')
        
        # 1. Scatter plot pr√©dictions vs r√©el
        ax = axes[0, 0]
        for name, result in self.results['regression'].items():
            ax.scatter(result['actual'], result['predictions'], alpha=0.6, label=name)
        
        # Ligne de r√©gression parfaite
        min_val = min([min(result['actual']) for result in self.results['regression'].values()])
        max_val = max([max(result['actual']) for result in self.results['regression'].values()])
        ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)
        
        ax.set_xlabel('Valeurs R√©elles')
        ax.set_ylabel('Pr√©dictions')
        ax.set_title('Pr√©dictions vs R√©el')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. Distribution des r√©sidus
        ax = axes[0, 1]
        for name, result in self.results['regression'].items():
            residuals = result['residuals']
            ax.hist(residuals, bins=30, alpha=0.7, label=name, density=True)
        
        ax.set_xlabel('R√©sidus')
        ax.set_ylabel('Densit√©')
        ax.set_title('Distribution des R√©sidus')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. Comparaison des m√©triques
        ax = axes[0, 2]
        metrics_df = pd.DataFrame([
            result['metrics'] for result in self.results['regression'].values()
        ], index=self.results['regression'].keys())
        
        metrics_df[['r2', 'rmse', 'mae']].plot(
            kind='bar', ax=ax, color=['#1f77b4', '#ff7f0e', '#2ca02c']
        )
        ax.set_title('Comparaison des M√©triques')
        ax.set_ylabel('Score')
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3)
        
        # 4. Q-Q plot des r√©sidus (premier mod√®le)
        ax = axes[1, 0]
        first_model = list(self.results['regression'].keys())[0]
        residuals = self.results['regression'][first_model]['residuals']
        
        from scipy import stats
        stats.probplot(residuals, dist="norm", plot=ax)
        ax.set_title(f'Q-Q Plot des R√©sidus - {first_model}')
        ax.grid(True, alpha=0.3)
        
        # 5. R√©sidus vs Pr√©dictions
        ax = axes[1, 1]
        for name, result in self.results['regression'].items():
            ax.scatter(result['predictions'], result['residuals'], alpha=0.6, label=name)
        
        ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)
        ax.set_xlabel('Pr√©dictions')
        ax.set_ylabel('R√©sidus')
        ax.set_title('R√©sidus vs Pr√©dictions')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 6. Feature Importance (premier mod√®le)
        ax = axes[1, 2]
        first_model = list(self.results['regression'].keys())[0]
        model = self.models_data[first_model]['model']
        
        if hasattr(model, 'feature_importances_'):
            importance = model.feature_importances_
            top_features = np.argsort(importance)[-10:]
            
            ax.barh(range(len(top_features)), importance[top_features])
            ax.set_yticks(range(len(top_features)))
            ax.set_yticklabels([self.feature_names[i] for i in top_features])
            ax.set_xlabel('Importance')
            ax.set_title(f'Feature Importance - {first_model}')
        
        plt.tight_layout()
        plt.savefig('regression_evaluation.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return fig
    
    def create_interactive_dashboard(self):
        """Cr√©e un dashboard interactif avec Plotly"""
        print("üé® Cr√©ation du dashboard interactif...")
        
        # Dashboard de classification
        if 'classification' in self.results:
            fig_clf = make_subplots(
                rows=2, cols=3,
                subplot_titles=('Courbes ROC', 'Precision-Recall', 'M√©triques',
                              'Distribution Probabilit√©s', 'Comparaison Mod√®les', 'Feature Importance'),
                specs=[[{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}],
                       [{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}]]
            )
            
            # Courbes ROC
            for name, result in self.results['classification'].items():
                fpr, tpr = result['roc_curve']
                auc = result['metrics']['roc_auc']
                fig_clf.add_trace(
                    go.Scatter(x=fpr, y=tpr, name=f'{name} (AUC={auc:.3f})', mode='lines'),
                    row=1, col=1
                )
            
            # Ligne de r√©f√©rence
            fig_clf.add_trace(
                go.Scatter(x=[0, 1], y=[0, 1], name='Random', mode='lines', 
                          line=dict(dash='dash', color='black')),
                row=1, col=1
            )
            
            # Distribution des probabilit√©s
            for name, result in self.results['classification'].items():
                probs = result['probabilities']
                fig_clf.add_trace(
                    go.Histogram(x=probs, name=name, opacity=0.7, nbinsx=30),
                    row=2, col=1
                )
            
            # Comparaison des m√©triques
            metrics_data = []
            for name, result in self.results['classification'].items():
                metrics_data.append({
                    'Model': name,
                    'ROC-AUC': result['metrics']['roc_auc'],
                    'Accuracy': result['metrics']['accuracy'],
                    'Precision': result['metrics']['precision'],
                    'Recall': result['metrics']['recall'],
                    'F1-Score': result['metrics']['f1_score']
                })
            
            metrics_df = pd.DataFrame(metrics_data)
            
            fig_clf.add_trace(
                go.Bar(x=metrics_df['Model'], y=metrics_df['ROC-AUC'], name='ROC-AUC'),
                row=1, col=3
            )
            
            fig_clf.update_layout(
                title_text="Dashboard d'√âvaluation - Classification",
                showlegend=True,
                height=800
            )
            
            fig_clf.show()
        
        # Dashboard de r√©gression
        if 'regression' in self.results:
            fig_reg = make_subplots(
                rows=2, cols=3,
                subplot_titles=('Pr√©dictions vs R√©el', 'R√©sidus', 'M√©triques',
                              'Q-Q Plot', 'R√©sidus vs Pr√©dictions', 'Feature Importance'),
                specs=[[{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}],
                       [{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}]]
            )
            
            # Scatter plot pr√©dictions vs r√©el
            for name, result in self.results['regression'].items():
                fig_reg.add_trace(
                    go.Scatter(x=result['actual'], y=result['predictions'], 
                              mode='markers', name=name, opacity=0.6),
                    row=1, col=1
                )
            
            # Distribution des r√©sidus
            for name, result in self.results['regression'].items():
                fig_reg.add_trace(
                    go.Histogram(x=result['residuals'], name=name, opacity=0.7, nbinsx=30),
                    row=1, col=2
                )
            
            # Comparaison des m√©triques
            metrics_data = []
            for name, result in self.results['regression'].items():
                metrics_data.append({
                    'Model': name,
                    'R¬≤': result['metrics']['r2'],
                    'RMSE': result['metrics']['rmse'],
                    'MAE': result['metrics']['mae']
                })
            
            metrics_df = pd.DataFrame(metrics_data)
            
            fig_reg.add_trace(
                go.Bar(x=metrics_df['Model'], y=metrics_df['R¬≤'], name='R¬≤'),
                row=1, col=3
            )
            
            fig_reg.update_layout(
                title_text="Dashboard d'√âvaluation - R√©gression",
                showlegend=True,
                height=800
            )
            
            fig_reg.show()
    
    def analyze_performance_by_segment(self, segment_col):
        """Analyse la performance par segment"""
        print(f"üìä Analyse de performance par {segment_col}...")
        
        if segment_col not in self.X_test.columns:
            print(f"‚ùå Colonne {segment_col} non trouv√©e")
            return
        
        segment_analysis = {}
        
        for segment in self.X_test[segment_col].unique():
            mask = self.X_test[segment_col] == segment
            y_segment = self.y_test[mask]
            
            if len(y_segment) < 10:  # Trop peu d'√©chantillons
                continue
            
            segment_metrics = {}
            
            # M√©triques de classification
            if 'classification' in self.results:
                for name, result in self.results['classification'].items():
                    y_pred_segment = result['predictions'][mask]
                    y_pred_proba_segment = result['probabilities'][mask]
                    
                    segment_metrics[f'{name}_auc'] = roc_auc_score(y_segment, y_pred_proba_segment)
                    segment_metrics[f'{name}_accuracy'] = (y_pred_segment == y_segment).mean()
            
            # M√©triques de r√©gression
            if 'regression' in self.results:
                for name, result in self.results['regression'].items():
                    y_pred_segment = result['predictions'][mask]
                    
                    segment_metrics[f'{name}_r2'] = r2_score(y_segment, y_pred_segment)
                    segment_metrics[f'{name}_rmse'] = np.sqrt(mean_squared_error(y_segment, y_pred_segment))
            
            segment_analysis[segment] = segment_metrics
        
        return segment_analysis
    
    def generate_comprehensive_report(self):
        """G√©n√®re un rapport complet"""
        print("üìã G√©n√©ration du rapport complet...")
        
        report = {
            'summary': {
                'total_models': len(self.models_data),
                'classification_models': len(self.results.get('classification', {})),
                'regression_models': len(self.results.get('regression', {}))
            },
            'classification_results': {},
            'regression_results': {},
            'recommendations': []
        }
        
        # R√©sultats de classification
        if 'classification' in self.results:
            best_clf = None
            best_clf_score = 0
            
            for name, result in self.results['classification'].items():
                metrics = result['metrics']
                report['classification_results'][name] = metrics
                
                if metrics['roc_auc'] > best_clf_score:
                    best_clf_score = metrics['roc_auc']
                    best_clf = name
            
            if best_clf:
                report['recommendations'].append(f"Meilleur mod√®le de classification: {best_clf} (AUC={best_clf_score:.4f})")
        
        # R√©sultats de r√©gression
        if 'regression' in self.results:
            best_reg = None
            best_reg_score = 0
            
            for name, result in self.results['regression'].items():
                metrics = result['metrics']
                report['regression_results'][name] = metrics
                
                if metrics['r2'] > best_reg_score:
                    best_reg_score = metrics['r2']
                    best_reg = name
            
            if best_reg:
                report['recommendations'].append(f"Meilleur mod√®le de r√©gression: {best_reg} (R¬≤={best_reg_score:.4f})")
        
        # Recommandations g√©n√©rales
        if 'classification' in self.results:
            avg_auc = np.mean([result['metrics']['roc_auc'] for result in self.results['classification'].values()])
            if avg_auc > 0.85:
                report['recommendations'].append("Performance excellente en classification (>0.85 AUC)")
            elif avg_auc > 0.75:
                report['recommendations'].append("Performance bonne en classification (>0.75 AUC)")
            else:
                report['recommendations'].append("Performance √† am√©liorer en classification (<0.75 AUC)")
        
        if 'regression' in self.results:
            avg_r2 = np.mean([result['metrics']['r2'] for result in self.results['regression'].values()])
            if avg_r2 > 0.80:
                report['recommendations'].append("Performance excellente en r√©gression (>0.80 R¬≤)")
            elif avg_r2 > 0.60:
                report['recommendations'].append("Performance bonne en r√©gression (>0.60 R¬≤)")
            else:
                report['recommendations'].append("Performance √† am√©liorer en r√©gression (<0.60 R¬≤)")
        
        return report
    
    def run_complete_evaluation(self):
        """Ex√©cute l'√©valuation compl√®te"""
        print("üöÄ D√©marrage de l'√©valuation compl√®te...")
        
        # √âvaluer les mod√®les
        self.evaluate_classification_models()
        self.evaluate_regression_models()
        
        # Cr√©er les visualisations
        self.create_classification_visualizations()
        self.create_regression_visualizations()
        
        # Cr√©er le dashboard interactif
        self.create_interactive_dashboard()
        
        # G√©n√©rer le rapport
        report = self.generate_comprehensive_report()
        
        print("‚úÖ √âvaluation compl√®te termin√©e!")
        return report

if __name__ == "__main__":
    # Test du module
    print("üß™ Test du dashboard d'√©valuation...") 